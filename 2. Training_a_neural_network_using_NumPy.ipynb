{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network for the MNIST dataset using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import read_mnist, show\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training data just for reference\n",
    "X_train, Y_train = read_mnist('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEKCAYAAADdIIPUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+M0y5fCbZcrhN8vU/wHobEu1FMR4PAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b6d788550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X_train[0], Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_prediction(x, W, b):\n",
    "    \"\"\"This is the entire neural net!\"\"\"\n",
    "    ##\n",
    "    ## your function from the last notebook\n",
    "    ##\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Return the softmax of vector z.\n",
    "\n",
    "    The softmax returns normalized positive values.\n",
    "    It is defined as softmax(z_i) = normalise(exp(z_i))\n",
    "    = \\\\frac{exp(z_i)}{\\sum_j exp(z_j)}.\n",
    "    \"\"\"\n",
    "    z -= np.max(z)  # for numerical stability\n",
    "    exps = np.exp(z)\n",
    "    return np.divide(exps, np.sum(exps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function for the network to minimise\n",
    "Also sometimes called a cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(p, q):\n",
    "    \"\"\"Return the cross-entropy of q with respect to p.\n",
    "\n",
    "    The cross-entropy is a measure of how much distribution q\n",
    "    diverges from distribution p. It is defined as\n",
    "    H_p(q) = - \\sum_x p(x) \\log q(x).\n",
    "    \"\"\"\n",
    "    return -np.sum(p * np.log(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode true labels as vectors, consistent with predictions\n",
    "The neural network takes a `28x28` pixel image (flattened to a vector of length 784) and outputs a prediction for digit shown in the image. The output is a vector of length ten, which forms a probability distribution over the possible classes `0-9` for the input image. Obviously the predicited class is just the class assigned the largest probability. \n",
    "\n",
    "But our loss function wants to compare distributions, and so expects vectors of equal length. Currently the true labels in the dataset are digits, `0`, or `1`, or ... `9`, not vectors. **Let's encode labels as a vector, such that `3` -> `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    \"\"\"One-hot encode a class label.\n",
    "\n",
    "    Given a class label, return an array of length 10 with all\n",
    "    elements zero, except for the element with index `label`,\n",
    "    which is 1.0.\n",
    "    \"\"\"\n",
    "    encoded = np.zeros(10)\n",
    "    encoded[label] = 1.0\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the gradients of the loss function with reference to parameters\n",
    "We 'learn' by updating the parameters W, b so as to minimise the loss function. This requires calculating the gradient of the loss function wrt the parameters.\n",
    "\n",
    "In general, when we have many millions of parameters the 'backpropagation' algorithm allows us to do this efficiently: we compute partial derivatives and apply the chain rule as needed to avoid unecessary recomputing. For this simple network we don't even need to worry about backprop.\n",
    "\n",
    "**Let's just code by hand the gradients for the loss with reference to W and b.**\n",
    "\n",
    "_(We did the maths so you don't have to. Feel free to check it, but obviously in the future you won't be hardcoding derivatives)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dloss_dw(y_true, x, y_pred):\n",
    "    \"\"\"Analytic gradient of cross-entropy with reference to matrix W.\"\"\"\n",
    "    return np.outer(np.negative(x), y_true*(1-y_pred))\n",
    "\n",
    "\n",
    "def dloss_db(y_true, x, y_pred):\n",
    "    \"\"\"Analytic gradient of cross-entropy with reference to vector b\"\"\"\n",
    "    return np.negative(y_true)*(1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the error rate\n",
    "Just like before, we'll want to see how our network performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate_mnist(W, b, dataset='test'):\n",
    "    \"\"\"Calculate the error rate.\"\"\"\n",
    "    data_seen = 0.\n",
    "    total_correct = 0.\n",
    "    total_errors = 0.\n",
    "\n",
    "    X, Y = read_mnist(dataset)\n",
    "    for x, y in zip(X, Y):\n",
    "        y_pred = np.argmax(neural_network_prediction(x, W, b))\n",
    "        if y_pred == y:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_errors += 1\n",
    "        data_seen += 1\n",
    "\n",
    "    return total_errors / data_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "To learn the parameters, we i. loop through the data, ii. compute the performance of the network and iii. update the parameters accordingly... we also iv. print out some metrics to see how we're doing.\n",
    "\n",
    "We provide you with the structure for a `learn` function your task is to make it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(W, b, max_iters=100000, learning_rate=0.01, batch_size=20):\n",
    "    \"\"\"Update W, b to reduce cross-entropy on using gradient descent.\"\"\"\n",
    "    W = W.copy()  # don't overwrite original parameters\n",
    "    b = b.copy()\n",
    "    grad_w = np.zeros(W.shape)  # initailise the gradients\n",
    "    grad_b = np.zeros(b.shape)\n",
    "\n",
    "    # keep track of some metrics\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    start = time.time()\n",
    "    print_freq = max_iters / 10  # print progress 10 times during learning\n",
    "\n",
    "    # learn by cycling repeatedly through data for max_iters iterations\n",
    "    X_train, Y_train = read_mnist('train')\n",
    "    \n",
    "    for i, (x, label) in enumerate(itertools.cycle(zip(X_train, Y_train))):\n",
    "        ## Task 1: generate prediction\n",
    "        ##\n",
    "        ##\n",
    "        ##\n",
    "        ## Task 2: compute loss and gradients\n",
    "        ##\n",
    "        ##\n",
    "        ##\n",
    "        \n",
    "        if i % batch_size == 0:\n",
    "            ## Task 3: update parameters every batch_size iterations\n",
    "            ##\n",
    "            ##\n",
    "            ##\n",
    "            ##\n",
    "            ##\n",
    "            ##\n",
    "            pass\n",
    "            \n",
    "        # print some metrics\n",
    "        if i % print_freq == 0:\n",
    "            total_loss /= print_freq\n",
    "            train_error = error_rate_mnist(W, b, 'training')\n",
    "            test_error = error_rate_mnist(W, b, 'validation')\n",
    "            running_time = (time.time() - start)\n",
    "            print((\"Iteration {i} | Loss: {loss:.4f} | \"\n",
    "                   \"Train error: {train:.4f} | Test error: {test:.4f} | \"\n",
    "                   \"Total run time: {t:.1f}s\".format(i=i,\n",
    "                                                     loss=total_loss,\n",
    "                                                     train=train_error,\n",
    "                                                     test=test_error,\n",
    "                                                     t=running_time)))\n",
    "            # accumulate the metrics\n",
    "            train_errors.append(train_error)\n",
    "            test_errors.append(test_error)\n",
    "            losses.append(total_loss)\n",
    "            total_loss = 0\n",
    "\n",
    "        if i > max_iters:\n",
    "            return W, b, losses, train_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Task 4: randomly initialise parameters\n",
    "# W1 = \n",
    "# b1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Task 5: Train your network!\n",
    "##\n",
    "##\n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning(losses, train_errors, test_errors):\n",
    "    '''Plot the train/test error rate and the loss'''\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(14, 5), squeeze=True)\n",
    "\n",
    "    ax[0].plot(range(len(train_errors)), train_errors, '-o', label='Training error')\n",
    "    ax[0].plot(range(len(test_errors)),  test_errors, '-o', label='Test error')\n",
    "    ax[0].set_ylim(0, 1)\n",
    "    ax[0].legend(loc='upper right')\n",
    "    ax[0].set_title('Error rate')\n",
    "\n",
    "    ax[1].plot(range(len(losses)), losses, '-o')\n",
    "    ax[1].set_title('Losses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Task 6: Plot the loss function and error rate over time\n",
    "##\n",
    "##"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
