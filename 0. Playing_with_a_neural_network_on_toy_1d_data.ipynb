{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Run all cells in Part 1 Defining Models _understanding the code in this section is not required._\n",
    "\n",
    "2. One by one, run the cells in Part 2 Fitting Models and try to understand the output of the Linear regression model, Logistic regression model and Neural network model sections\n",
    "\n",
    "3. Play with the learn_rate, n_iters, n_hidden_units parameters from Part 2 to see how they affect the outcomes above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Defining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Theano\n",
    "Theano is library with helpful functionality for building neural networks, like automatic differentiation. \n",
    "**In this course, you will not need to understand any theano.**\n",
    "\n",
    "We use Theano in some of the functions in this notebook for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the hard-coded data used for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x values of the data points we will use\n",
    "x_values = np.arange(-1,1,.05)\n",
    "\n",
    "# y values of the data points we will use\n",
    "y_noise = np.asarray([ 0.19200746,  0.26390769,  0.27416557,  0.29274981,  0.34720758,\n",
    "        0.33646458,  0.3954851 ,  0.38163466,  0.4721299 ,  0.4648484 ,\n",
    "        0.45125321,  0.49003209,  0.4403167 ,  0.46711053,  0.48810206,\n",
    "        0.459813  ,  0.46432161,  0.52867192,  0.55053968,  0.50073061,\n",
    "        0.50371526,  0.50858915,  0.49070523,  0.5121518 ,  0.52849574,\n",
    "        0.51643671,  0.44317211,  0.51967805,  0.49736733,  0.49924654,\n",
    "        0.48524065,  0.50282076,  0.56767256,  0.50839501,  0.57744206,\n",
    "        0.58775349,  0.62222579,  0.57015087,  0.63695049,  0.64991875])\n",
    "\n",
    "# These are 'true' y values from the underlying distribution with no noise\n",
    "y_true = np.asarray([ 0.2       ,  0.24053125,  0.27725   ,  0.31034375,  0.34      ,\n",
    "        0.36640625,  0.38975   ,  0.41021875,  0.428     ,  0.44328125,\n",
    "        0.45625   ,  0.46709375,  0.476     ,  0.48315625,  0.48875   ,\n",
    "        0.49296875,  0.496     ,  0.49803125,  0.49925   ,  0.49984375,\n",
    "        0.5       ,  0.49990625,  0.49975   ,  0.49971875,  0.5       ,\n",
    "        0.50078125,  0.50225   ,  0.50459375,  0.508     ,  0.51265625,\n",
    "        0.51875   ,  0.52646875,  0.536     ,  0.54753125,  0.56125   ,\n",
    "        0.57734375,  0.596     ,  0.61740625,  0.64175   ,  0.66921875])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualization of the data used in this notebook\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(x_values, y_true)\n",
    "plt.plot(x_values, y_noise, '.', color='darkorange')\n",
    "plt.title('Underlying distribution and sampled data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the 3 different models we will consider\n",
    "As discussed in the lecture, we define three classes of models: \n",
    "1. linear regression, \n",
    "2. logistic regression, and \n",
    "3. a feed-forward neural network. \n",
    "\n",
    "**They are all essentially the same models, but with increasing non-linearity and increasing parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model is y_pred = m*x + b\n",
    "class Linear_Regression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        learning_rate = T.dscalar('learning_rate') \n",
    "        x = T.dvector('x')\n",
    "        y = T.dvector('y')\n",
    "        \n",
    "        # Randomly initialize the model parameters \n",
    "        # theano.shared() is a technicality \n",
    "        self.m = theano.shared(np.random.randn(), name='m')\n",
    "        self.b = theano.shared(np.random.randn(), name='b')       \n",
    "        \n",
    "        # model's prediction, given x and parameters\n",
    "        y_pred = T.dot(self.m,x)  + self.b\n",
    "        \n",
    "        # cost function we will minimize (mean squared error)\n",
    "        cost = T.sum(T.pow(y_pred - y, 2)) / (2 * x.shape[0])\n",
    "\n",
    "        # calculate the gradient of the cost function wrt the parameters\n",
    "        gradient = T.grad(cost, [self.m, self.b])\n",
    "        \n",
    "        # define the gradient descent updates to minimize the cost function        \n",
    "        updates = ([self.m, self.m - learning_rate*gradient[0]],\n",
    "                   [self.b, self.b - learning_rate*gradient[1]])\n",
    "        \n",
    "        # function to take a single gradient step for the minimization\n",
    "        self.train = theano.function(inputs=[x, y, learning_rate],\n",
    "                                     outputs=cost,\n",
    "                                     updates=updates)\n",
    "\n",
    "        # make a prediction with the model (outputs y_pred)        \n",
    "        self.predict = theano.function(inputs=[x], \n",
    "                                       outputs=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model is y_pred = sigmoid(m*x + b)\n",
    "class Logistic_Regression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        learning_rate = T.dscalar('learning_rate')\n",
    "        x = T.dvector('x')\n",
    "        y = T.dvector('y')\n",
    "        \n",
    "        # initialize parameters \n",
    "        self.m = theano.shared(np.random.randn(), name = 'm')\n",
    "        self.b = theano.shared(np.random.randn(), name = 'b')       \n",
    "\n",
    "        # prediction\n",
    "        y_pred = T.nnet.sigmoid(T.dot(self.m, x) + self.b)\n",
    "        \n",
    "        # cost function \n",
    "        cost = T.sum(T.pow(y_pred - y, 2)) / (2 * x.shape[0])\n",
    "\n",
    "        # gradient of the cost function wrt the parameters\n",
    "        gradient = T.grad(cost, [self.m, self.b])\n",
    "        \n",
    "        # gradient descent updates\n",
    "        updates = ([self.m, self.m - learning_rate*gradient[0]],\n",
    "                   [self.b, self.b - learning_rate*gradient[1]])\n",
    "\n",
    "        # single gradient step \n",
    "        self.train = theano.function(inputs=[x, y, learning_rate],\n",
    "                                     outputs=cost,\n",
    "                                     updates=updates)\n",
    "        \n",
    "        # make a prediction with the model (outputs y_pred)        \n",
    "        self.predict = theano.function(inputs=[x],\n",
    "                                       outputs=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model is y_pred = tanh(W2*tanh(W1*x + b1) + b2)\n",
    "class Feedforward_Network_1D:\n",
    "    \n",
    "    def __init__(self, n_hidden_units=100):\n",
    "        norm = np.sqrt(1. / n_hidden_units)\n",
    "        learning_rate = T.dscalar('learning_rate')\n",
    "        x = T.dvector('x')\n",
    "        y = T.dvector('y')\n",
    "        \n",
    "        # Randomly initialize the model parameters\n",
    "        self.W1 = theano.shared(np.random.randn(n_hidden_units) * norm,\n",
    "                                name='W1')\n",
    "        self.b1 = theano.shared(np.random.randn(n_hidden_units, 1) * norm,\n",
    "                                name='b1', broadcastable=(False, True))\n",
    "        self.W2 = theano.shared(np.random.randn(n_hidden_units) * norm,\n",
    "                                name='W2')\n",
    "        self.b2 = theano.shared(np.random.randn(1) * norm,\n",
    "                                name='b2')\n",
    "\n",
    "        # This is the \"hidden layer\" in the model \n",
    "        # it is just a way to breakdown a non-linear function\n",
    "        h = T.tanh(T.outer(self.W1, x) + self.b1)\n",
    "\n",
    "        # This is the model's prediction, given x, and the parameters        \n",
    "        y_pred = T.tanh(T.dot(self.W2, h) + self.b2.dimshuffle(0, 'x'))\n",
    "        \n",
    "        # This is cost function we will minimize (mean squared error of the model)\n",
    "        cost = T.sum(T.pow(y_pred - y, 2)) / (2 * x.shape[0])\n",
    "        \n",
    "        # Here we calculate the gradient of the cost function wrt the parameters        \n",
    "        gradient = T.grad(cost, [self.W1, self.b1, self.W2, self.b2])\n",
    "        \n",
    "        # Here we define the gradient descent updates to minimize the cost function        \n",
    "        updates = ([self.W1, self.W1 - learning_rate*gradient[0]],\n",
    "                   [self.b1, self.b1 - learning_rate*gradient[1]], \n",
    "                   [self.W2, self.W2 - learning_rate*gradient[2]],\n",
    "                   [self.b2, self.b2 - learning_rate*gradient[3]])\n",
    "\n",
    "        # This function takes a single gradient step for the minimization        \n",
    "        self.train = theano.function(inputs=[x, y, learning_rate],\n",
    "                                     outputs=cost,\n",
    "                                     updates=updates)\n",
    "        \n",
    "        # This function makes a prediction with the model (outputs y_pred)      \n",
    "        self.predict = theano.function(inputs=[x],\n",
    "                                       outputs=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define minimization algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the minimization function for all models\n",
    "def learn(model, n_iters=100000, learn_rate=0.1):\n",
    "    '''Learn a model to minimise the model's cost function.'''\n",
    "    \n",
    "    # track some metrics\n",
    "    print_freq = int(n_iters/10.)\n",
    "    cost  = 0.0\n",
    "    costs = []\n",
    "    iters= []\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        cost += model.train(x_values, y_noise, learn_rate)\n",
    "        \n",
    "        if (i % print_freq == 0 and i > 0) or i == n_iters-1:  # print metrics\n",
    "            print(\"At iteration %d, cost equals: %.6f, Total run time = %.2f mins\" \n",
    "                  % (i, cost/print_freq, (time.time()-start)/60.))\n",
    "            costs.append(cost/print_freq)\n",
    "            iters.append(i)\n",
    "            cost = 0\n",
    "    \n",
    "    return iters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to visualise learning and performanace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cost(iters, costs):\n",
    "    '''Plot the cost over iterations.'''\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.plot(iters, costs)\n",
    "    plt.title('Cost function vs learning-iteration step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_prediction(model):\n",
    "    '''Plot predictions versus data.'''\n",
    "    predict_array = np.ndarray(len(x_values))\n",
    "    predict_array = model.predict(x_values).reshape(len(x_values),)\n",
    "\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.plot(x_values, predict_array,  color='darkblue', label='Model prediction')\n",
    "    plt.plot(x_values, y_true, '-', color='darkgreen', label='Underlying distribution')\n",
    "    plt.plot(x_values, y_noise,'.', color='darkorange', label='Training data')\n",
    "    plt.title('Model prediction compared to true underlying distribution')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fitting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the linear-regression model\n",
    "linear_regression_model = Linear_Regression()\n",
    "\n",
    "# Learn the models parameters\n",
    "linear_regression_iters, linear_regression_costs = learn(linear_regression_model, \n",
    "                                                         n_iters=1000, \n",
    "                                                         learn_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the results of the learned linear-regression model\n",
    "plot_cost(linear_regression_iters, linear_regression_costs)\n",
    "plot_prediction(linear_regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the logistic-regression model\n",
    "logistic_regression_model = Logistic_Regression()\n",
    "\n",
    "# Learn the models parameters\n",
    "logistic_regression_iters, logistic_regression_costs = learn(logistic_regression_model, \n",
    "                                                             n_iters=20000, \n",
    "                                                             learn_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the results of the learned logistic-regression model\n",
    "plot_cost(logistic_regression_iters, logistic_regression_costs)\n",
    "plot_prediction(logistic_regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the feed-forward neural network model\n",
    "feedforward_network_1d_model = Feedforward_Network_1D(n_hidden_units=100)\n",
    "\n",
    "# Learn the models parameters\n",
    "feedforward_network_1d_iters, feedforward_network_1d_costs = learn(feedforward_network_1d_model, \n",
    "                                                                   n_iters=100000, \n",
    "                                                                   learn_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the results of the learned feed-forward neural network model\n",
    "plot_cost(feedforward_network_1d_iters, feedforward_network_1d_costs)\n",
    "plot_prediction(feedforward_network_1d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play with the learn_rate, n_iters and n_hidden_units parameters from Part 2 to see how they affect the outcomes above"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
